{
    "Slug": "a-fast-serverless-big-data-pipeline-powered-by-a-single-azure-function",
    "Title": "A fast, serverless, big data pipeline powered by a single Azure Function",
    "Summary": "A single Azure function is all it took to fully implement an end-to-end, real-time, mission critical data pipeline. And it was done with a serverless architecture. Serverless architectures simplify the building, deployment, and management of cloud scale applications.",
    "Content": "<p>A single Azure function is all it took to fully implement an end-to-end, real-time, mission critical data pipeline. And it was done with a <a href=\"https://azure.microsoft.com/en-us/overview/serverless-computing/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">serverless</a> architecture. Serverless architectures simplify the building, deployment, and management of cloud scale applications. Instead of worrying about data infrastructure like server procurement, configuration, and management a data engineer can focus on the tasks it takes to ensure an end-to-end and highly functioning data pipeline.</p>\n\n<p>This blog describes an Azure function and how it efficiently coordinated a data ingestion pipeline that processed over eight million transactions per day.</p>\n\n<h2>Scenario</h2>\n\n<p>A large bank wanted to build a solution to detect fraudulent transactions submitted through mobile phone banking applications. The solution requires a big data pipeline approach. High volumes of real-time data are ingested into a cloud service, where a series of data transformation and extraction activities occur. This results in the creation of a <a href=\"https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/create-features/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">feature</a> data set, and the use of advanced analytics. For the bank, the pipeline had to be very fast and scalable, end-to-end evaluation of each transaction had to complete in less than two seconds.</p>\n\n<p>Telemetry from the bank&rsquo;s multiple application gateways, stream in as embedded events in complex JSON files. The ingestion technology is Azure Event Hubs. Each event is ingested into an Event Hub and parsed into multiple individual transactions. Attributes are extracted from each transaction and evaluated for fraud. The serverless architecture is constructed from these Azure services:</p>\n\n<ul>\n <li><a href=\"https://azure.microsoft.com/en-us/services/event-hubs/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Azure Event Hubs</a></li>\n <li><a href=\"https://azure.microsoft.com/en-us/services/functions/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Azure Functions</a></li>\n <li><a href=\"https://studio.azureml.net/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Azure Machine Learning Studio</a></li>\n <li><a href=\"https://azure.microsoft.com/en-us/services/sql-database/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Azure SQL Database</a></li>\n <li><a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">CosmosDB</a></li>\n</ul>\n\n<h2>Pipeline architecture</h2>\n\n<p>A single Azure Function was used to orchestrate and manage the entire pipeline of activities. The following diagram highlights the Azure Functions pipeline architecture:</p>\n\n<ul>\n <li>An enterprise system bus sends bank transaction in a JSON file that arrives into an Event Hub. The arrival triggers a response to validate and parse the ingested file.</li>\n <li>A SQL stored procedure is invoked. The procedure extracts data elements from the JSON message and aggregates them with customer and account profiles to generate a feature set, which is the input for a machine learning model. The aggregated message is formatted as a JSON file.</li>\n <li>The validated JSON message is written to Cosmos DB.</li>\n <li>A machine learning model is invoked to evaluate and score the transaction.</li>\n <li>The fraud score is posted back to an on-premises API for integration to a case management solution.</li>\n</ul>\n\n<p>&nbsp;<a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/416c10f2-3876-48da-bba3-f3f6d993a334.png\"><img alt=\"image\" border=\"0\" height=\"226\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/78320868-0fa7-4f1c-b0eb-3d3a68ef52bc.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"image\" width=\"377\"></a></p>\n\n<p align=\"center\"><em>Figure 1: Azure Function pipeline architecture</em>&nbsp;</p>\n\n<h2>Pipeline in ten steps</h2>\n\n<p>The Azure Function is written in C# and is composed of ten methods that are charted out in the diagram that follows.&nbsp; The methods include:</p>\n\n<p>1. A method is triggered when an event is received by an Event Hub.</p>\n\n<pre>\npublic static void Run(string myEventHubMessage, ICollector&lt;string&gt; resultsCollection, TraceWriter log)</pre>\n\n<p>2. The message is processed, and the JSON is validated.</p>\n\n<pre>\nprivate static void ProcessInitialMessageFromEventHub(List&lt;string&gt; jsonResults, string cnnString, TelemetryClient appInsights, dynamic d)</pre>\n\n<p>3. Invoke code to execute a SQL command to insert&nbsp; the message event.</p>\n\n<pre>\nprivate static bool CheckRequestTypeForValidMessage(dynamic d)</pre>\n\n<p>4. If the JSON message is valid, save it to Cosmos DB for purposes of querying later.</p>\n\n<pre>\nprivate static void SaveDocDb(string json, TraceWriter log)</pre>\n\n<p>5. Once the JSON is parsed, extract the relevant attributes.</p>\n\n<pre>\nprivate static string ProcessSQLReturnedFeaturesForAML(TraceWriter log, List&lt;string&gt;, jsonResults, TelemetryClient appInsights)</pre>\n\n<p>6. Execute a stored procedure to create the features that will be the input to the machine learning model.</p>\n\n<pre>\nprivate static string SendDataToStoredProc(dynamic d, SqlCommand spCommand, dynamic t, TelemetryClient appInsights, TransactionType transactionTypeEnum = TransactionType.Other, dynamic responseData = null)</pre>\n\n<p>7. Invoke a call to the Azure ML services endpoint. Obtain a score from Azure ML. Pass in the input parameters.</p>\n\n<pre>\nprivate static string CallAzureMl(dynamic d, TraceWriter log, HttpClient client)</pre>\n\n<p>8. The ML service returns a score which is then processed.</p>\n\n<pre>\npublic static List&lt;string&gt; GetScoresFromAzureMl(string myEventHubMessage, TraceWriter log, TelemetryClient appInsights, HttpClient client)</pre>\n\n<p>9. Make a call to the on-premises system, passing the message as an argument.</p>\n\n<pre>\npublic static List&lt;string&gt; ProcessMessagesIntoEsb(TraceWriter log, string cnnString, TelemetryClient appInsights, string cardNumber, string accountNumber, List&lt;string&gt;esbReturnMessages)</pre>\n\n<p>10. The score is evaluated against a threshold, which determines if it should be passed on to the on-premises case management system.</p>\n\n<pre>\npublic static string CheckScoreAndProcessEsbMessages(string&gt; myEventHubMessage, TraceWriter log, SqlCommand spCommand, TelemetryClient appInsights, string cardNumber, string accountNumber)</pre>\n\n<p>The&nbsp; figure below shows the logic as a vertical set of ten blocks, one for each task in the code.</p>\n\n<p><a href=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/2bbd46f9-71cb-4671-a788-3ea5bbda15ae.png\"><img alt=\"image\" border=\"0\" height=\"367\" src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/03699ea0-5c37-41df-8044-277cf135ed41.png\" style=\"border: 0px currentcolor; border-image: none; margin-right: auto; margin-left: auto; float: none; display: block; background-image: none;\" title=\"image\" width=\"322\"></a></p>\n\n<p align=\"center\"><em>Figure 2: Azure Function pipeline flow</em>&nbsp;</p>\n\n<p>&nbsp;</p>\n\n<h2>Pipeline scalability</h2>\n\n<p>The pipeline must be responsive to extreme bursts of incoming JSON files. It must parse each file into individual transactions, and process and evaluate each transaction for fraud. After experimenting with different configuration parameters, there were some settings that were helpful to ensure the Azure function could scale as needed and process the volume of messages and transactions within the required time constraints:&nbsp;</p>\n\n<ul>\n <li><a href=\"https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-autoscale-get-started?toc=%2fazure%2fapp-service%2ftoc.json/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Azure Autoscale</a> is a capability built into cloud services like Azure Functions. It is rules-based and provides the ability to scale a service like Azure Functions up or down based on defined thresholds. By default, because of the volume of data ingested into Event Hubs, the Azure Functions service scaled too quickly, and created too many instances of itself. That resulted in locking issues on the Event Hub partitions, impacting throughput significantly. After experimentation with autoscale feature, the setting for the Functions service was set to a minimum of one and a maximum of four instances.</li>\n <li>Two Event Hubs settings were important for ensuring performance and throughput for the Azure function:<br>\n <strong>maxBatchSize</strong>: Gets or sets the maximum event count that a user is willing to accept for processing per receive loop. This count is on a per-Event Hub partition level.<br>\n <strong>prefetchCount</strong>: Gets or sets the number of events that any receiver in the currently owned partition will actively cache. The default value for this property is 300.</li>\n</ul>\n\n<p>After experimenting with different settings, the optimal configuration for this solution turned out to be:</p>\n\n<pre>\n// Configuration settings for &#39;eventHub&#39; triggers. (Optional)\n   &quot;eventHub&quot;: {\n    // The maximum event count received per receive loop. The default is 64.\n    &quot;maxBatchSize&quot;: 10,\n    // The default PrefetchCount that will be used by the underlying EventProcessorHost.\n    &quot;prefetchCount&quot;: 40,\n    // The number of event batches to process before creating an EventHub cursor   checkpoint.\n    &quot;batchCheckpointFrequency&quot;: 1\n   },</pre>\n\n<h2>Recommended next steps</h2>\n\n<p>With serverless architecture, a data engineering team can focus on data flows, application logic, and service integration.&nbsp; If you are designing a real-time, serverless data pipeline and want the flexibility of coding your own methods for either integration with other services or to deploy through continuous integration, consider using Azure Functions to orchestrate and manage the pipeline. Check out these resources for additional information about Azure functions:</p>\n\n<ul>\n <li><a href=\"https://docs.microsoft.com/en-us/azure/azure-functions/functions-best-practices/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Optimize the performance and reliability of Azure functions</a>.</li>\n <li><a href=\"https://docs.microsoft.com/en-us/azure/monitoring-and-diagnostics/monitoring-autoscale-get-started?toc=%2fazure%2fapp-service%2ftoc.json/?WT.mc_id=ms-docs-kbaroni\" target=\"_blank\">Get started with Autoscale in Azure</a>.</li>\n <li>The full architecture for the bank fraud solution referenced in this blog can be found here: <a href=\"https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdownload.microsoft.com%2Fdownload%2F0%2F1%2F5%2F0150425C-14C7-41F4-97EA-3DE57B678C51%2FIndSG_FraudDetection.pdf&amp;data=02%7C01%7C%7Cdb1f75f50e814905327b08d60c53078e%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636709948072813252&amp;sdata=sVMHwEWyZ%2FkpPWgNkfgmXk5TfB0tzOvkMabszoZVuH8%3D&amp;reserved=0\" target=\"_blank\">Mobile Bank Fraud Solution Guide</a>.</li>\n</ul>\n\n<p>Special thanks to <a href=\"https://www.linkedin.com/in/cedza/\" target=\"_blank\">Cedric Labuschagne</a>, <a href=\"https://www.linkedin.com/in/chris-cook-49b88457/\" target=\"_blank\">Chris Cook</a>, and <a href=\"https://www.linkedin.com/in/eujon-sellers/\" target=\"_blank\">Eujon Sellers</a> for their collaboration on this blog.</p>\n"
}