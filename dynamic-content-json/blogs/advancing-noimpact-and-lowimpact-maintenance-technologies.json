{
    "Slug": "advancing-noimpact-and-lowimpact-maintenance-technologies",
    "Title": "Advancing no-impact and low-impact maintenance technologies",
    "Summary": "This post continues our reliability series kicked off by my July blog post highlighting several initiatives underway to keep improving platform availability, as part of our commitment to provide a trusted set of cloud services.",
    "Content": "<p>&ldquo;This post continues our <a href=\"https://azure.microsoft.com/en-us/blog/tag/advancing-reliability/\" target=\"_blank\">reliability series</a> kicked off by my <a href=\"https://azure.microsoft.com/en-us/blog/advancing-microsoft-azure-reliability/\" target=\"_blank\">July blog post</a> highlighting several initiatives underway to keep improving platform availability, as part of our commitment to provide a trusted set of cloud services. Today I wanted to double-click on the investments we&rsquo;ve made in no-impact and low-impact update technologies including hot patching, memory-preserving maintenance, and live migration. We&rsquo;ve deployed dozens of security and reliability patches to host infrastructure in the past year, many of which were implemented with no customer impact or downtime. The post that follows was written by John Slack from our core operating systems team, who is the Program Manager for several of the update technologies discussed below.&rdquo; - Mark Russinovich, CTO, Azure</p>\n\n<hr>\n<p><em>This post was co-authored by Apurva Thanky, Cristina del Amo Casado, and Shantanu Srivastava from the engineering teams responsible for these technologies.</em></p>\n\n<p>&nbsp;</p>\n\n<p>We regularly update Azure host infrastructure to improve the reliability, performance, and security of the platform. While the purposes of these &lsquo;maintenance&rsquo; updates vary, they typically involve updating software components in the hosting environment or decommissioning hardware. If we go back five years, the only way to apply some of these updates was by fully rebooting the entire host. This approach took customer virtual machines (VMs) down for minutes at a time. Since then, we have invested in a variety of technologies to minimize customer impact when updating the fleet. Today, the vast majority of updates to the host operating system are deployed in place with absolute transparency and zero customer impact using hot patching. In infrequent cases in which the update cannot be hot patched, we typically utilize low-impact memory preserving update technologies to roll out the update.</p>\n\n<p>Even with these technologies, there are still other rare cases in which we need to do more impactful maintenance (including evacuating faulty hardware or decommissioning old hardware). In such cases, we use a combination of live migration, in-VM notifications, and planned maintenance providing customer controls.</p>\n\n<p>Thanks to continued investments in this space, we are at a point where the vast majority of host maintenance activities do not impact the VMs hosted on the affected infrastructure. We&rsquo;re writing this post to be transparent about the different techniques that we use to ensure that Azure updates are minimally impactful.</p>\n\n<h2>Plan A: Hot patching</h2>\n\n<p>Function-level &ldquo;hot&rdquo; patching provides the ability to make targeted changes to running code without incurring any downtime for customer VMs. It does this by redirecting all new invocations of a function on the host to an updated version of that function, so it is considered a &lsquo;no impact&rsquo; update technology. Wherever possible we use hot patching to apply host updates completely avoiding any impact to the VMs running on that host. We have been using hot patching in Azure since 2017. Since then, we have worked to broaden the scope of what we can hot patch. As an example, we updated the host operating system to allow the hypervisor to be hot patched in 2018. Looking forward, we are exploring firmware hot patches. This is a place where the industry typically hasn&#39;t focused. Firmware has always been viewed as &lsquo;if you need to update it, reboot the server,&rsquo; but we know that makes for a terrible customer experience. We&#39;ve been working with hardware manufacturers to consider our own firmware to make them hot patchable and incrementally updatable.</p>\n\n<p>Some large host updates contain changes that cannot be applied using function-level hot patching. For those updates, we endeavor to use memory-preserving maintenance.</p>\n\n<h2>Plan B: Memory-preserving maintenance</h2>\n\n<p>Memory-preserving maintenance involves &lsquo;pausing&rsquo; the guest VMs (while preserving their memory in RAM), updating the host server, then resuming the VMs and automatically synchronizing their clocks. We first used memory-preserving maintenance for Azure in 2018. Since then we have improved the technology in three important ways. First, we have developed less impactful variants of memory-preserving maintenance targeted for host components that can be serviced without a host reboot. Second, we have reduced the duration of the customer experienced pause. Third, we have expanded the number of VM types that can be updated with memory preserving maintenance. While we continue to work in this space, some variants of memory-preserving maintenance are still incompatible with some specialized VM offerings like M, N, or H series VMs for a variety of technical reasons.</p>\n\n<p>In the rare case we need to make more impactful maintenance (including host reboots, VM redeployment), customers are notified in advance and given the opportunity to perform the maintenance at a time suitable for their workload(s).</p>\n\n<h2>Plan C: Self-service maintenance</h2>\n\n<p>Self-service maintenance involves providing customers and partners a window of time, within which they can choose when to initiate impactful maintenance on their VM(s). This initial self-service phase typically lasts around a month and empowers organizations to perform the maintenance on their own schedules so it has no or minimal disruption to users. At the end of this self-service window, a scheduled maintenance phase begins&mdash;this is where Azure will perform the maintenance automatically. Throughout both phases, customers get full visibility of which VMs have or have not been updated&mdash;in Azure Service Health or by querying in PowerShell/CLI. Azure first offered self-service maintenance in 2018. We generally see that administrators take advantage of the self-service phase rather than wait for Azure to perform maintenance on their VMs automatically.</p>\n\n<p>In addition to this, when the customer owns the full host machine, either using <a href=\"https://azure.microsoft.com/en-us/services/virtual-machines/dedicated-host/\" target=\"_blank\">Azure Dedicated Hosts</a> or <a href=\"https://docs.microsoft.com/en-us/azure/security/fundamentals/isolation-choices#isolated-virtual-machine-sizes\" target=\"_blank\">Isolated virtual machines</a>, we recently started to offer maintenance control over all non-zero impact platform updates. This includes rebootless updates which only cause a few seconds pause. It is useful for VMs running ultra-sensitive workloads which can&rsquo;t sustain any interruption even if it lasts just for a few seconds. Customers can choose when to apply these non-zero impact updates in a 35-day rolling window. This feature is in public preview, and more information can be found in this dedicated <a href=\"https://azure.microsoft.com/en-us/blog/maintenance-control-for-platform-updates/\" target=\"_blank\">blog post</a>.</p>\n\n<p>Sometimes in-place update technologies aren&rsquo;t viable, like when a host shows signs of hardware degradation. In such cases, the best option is to initiate a move of the VM to another host, either through customer control via planned maintenance or through live migration.</p>\n\n<h2>Plan D: Live migration</h2>\n\n<p>Live migration involves moving a running customer VM from one &ldquo;source&rdquo; host to another &ldquo;destination&rdquo; host. Live migration starts by moving the VM&rsquo;s local state (including RAM and local storage) from the source to the destination while the virtual machine is still running. Once most of the local state is moved, the guest VM experiences a short pause usually lasting five seconds or less. After that pause, the VM resumes running on the destination host. Azure first started using live migration for maintenance in 2018. Today, when Azure Machine Learning algorithms predict an impending hardware failure, live migration can be used to move guest VMs onto different hosts <a href=\"https://azure.microsoft.com/blog/improving-azure-virtual-machine-resiliency-with-predictive-ml-and-live-migration/\" target=\"_blank\">preemptively</a>.</p>\n\n<p>Amongst other topics, planned maintenance and AI Operations were covered in Igal Figlin&rsquo;s recent Ignite 2019 session &ldquo;Building resilient applications in Azure.&rdquo; Watch the recording <a href=\"https://myignite.techcommunity.microsoft.com/sessions/82913?source=sessions\" target=\"_blank\">here</a> for additional context on these, and to learn more about how to take advantage of the various resilient services Azure provides to help you build applications that are inherently resilient.</p>\n\n<h2>The future of Azure maintenance</h2>\n\n<p><embed https:=\"\" myignite.techcommunity.microsoft.com=\"\" sessions=\"\" source=\"sessions\" video=\"\">In summary, the way in which Azure performs maintenance varies significantly depending on the type of updates being applied. Regardless of the specifics, Azure always approaches maintenance with a view towards ensuring the smallest possible impact to customer workloads. This post has outlined several of the technologies that we use to achieve this, and we are working diligently to continue improving the customer experience. As we look toward the future, we are investing heavily in machine learning-based insights and automation to maintain availability and reliability. Eventually, this &ldquo;AI Operations&rdquo; model will carry out preventative maintenance, initiate automated mitigations, and identify contributing factors and dependencies during incidents more effectively than our human engineers can. We look forward to sharing more on these topics as we continue to learn and evolve.</p>\n\n<p>&nbsp;</p>\n\n<p>&nbsp;</p>\n"
}